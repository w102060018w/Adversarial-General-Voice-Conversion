{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Dec  7 14:38:24 2017\n",
    "\n",
    "@author: huiting hong\n",
    "\n",
    "Edit on Sun Dec 9 20:11:17 2017\n",
    "@edit: Huiting Hong\n",
    "Edit into RNN-VAE\n",
    "Still existing some bugs to fix (dimension need to put more concern)\n",
    "Already test it, it could iterate pretty nice but the MSE result .... Ummm ... really suck.\n",
    "\"\"\"\n",
    "\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "#import scipy.io as sio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "def generate_random_sample(data):\n",
    "    while True:\n",
    "        random_mat = []\n",
    "        for i in range(BATCHSIZE):\n",
    "            random_index = np.random.choice(data.__len__(), size=1 ,replace = False)[0]\n",
    "            if random_index > data.__len__()-TIMESTEP:\n",
    "                random_index -= TIMESTEP\n",
    "            random_ary = list(range(random_index,random_index+TIMESTEP))\n",
    "            random_mat.append(data[random_ary,:])\n",
    "        random_mat = np.asarray(random_mat)\n",
    "        yield torch.from_numpy(random_mat).float()\n",
    "        \n",
    "        \n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, isCuda):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.isCuda = isCuda\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #initialize weights\n",
    "        nn.init.xavier_uniform(self.lstm.weight_ih_l0, gain=np.sqrt(2))\n",
    "        nn.init.xavier_uniform(self.lstm.weight_hh_l0, gain=np.sqrt(2))\n",
    "\n",
    "    def forward(self, input):\n",
    "        tt = torch.cuda if self.isCuda else torch\n",
    "        h0 = Variable(tt.FloatTensor(self.num_layers, input.size(0), self.hidden_size))\n",
    "        c0 = Variable(tt.FloatTensor(self.num_layers, input.size(0), self.hidden_size))\n",
    "        encoded_input, hidden = self.lstm(input, (h0, c0))\n",
    "        encoded_input = self.relu(encoded_input)\n",
    "        return encoded_input\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, isCuda):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.isCuda = isCuda\n",
    "        self.lstm = nn.LSTM(hidden_size, output_size, num_layers, batch_first=True)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        #initialize weights\n",
    "        nn.init.xavier_uniform(self.lstm.weight_ih_l0, gain=np.sqrt(2))\n",
    "        nn.init.xavier_uniform(self.lstm.weight_hh_l0, gain=np.sqrt(2))\n",
    "        \n",
    "    def forward(self, encoded_input):\n",
    "        tt = torch.cuda if self.isCuda else torch\n",
    "        h0 = Variable(tt.FloatTensor(self.num_layers, encoded_input.size(0), self.output_size))\n",
    "        c0 = Variable(tt.FloatTensor(self.num_layers, encoded_input.size(0), self.output_size))\n",
    "        decoded_output, hidden = self.lstm(encoded_input, (h0, c0))\n",
    "        decoded_output = self.sigmoid(decoded_output)\n",
    "        return decoded_output\n",
    "\n",
    "class LSTMAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, isCuda):\n",
    "        super(LSTMAE, self).__init__()\n",
    "        self.encoder = EncoderRNN(input_size, hidden_size, num_layers, isCuda)\n",
    "        self.decoder = DecoderRNN(hidden_size, input_size, num_layers, isCuda)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        encoded_input = self.encoder(input)\n",
    "        decoded_output = self.decoder(encoded_input)\n",
    "        return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% parameters\n",
    "ALLITER = 10000\n",
    "VAEITER = 10000\n",
    "BATCHSIZE = 64\n",
    "LAMBDAS = 1\n",
    "ALPHAS = 0\n",
    "\n",
    "TYPE = 'lstm_dense_PURE_VAE_remove_Sample_TEST_ON_ITER'\n",
    "PLOT = True\n",
    "GENERATE = True\n",
    "TIMESTEPS = 50\n",
    "LSTM_HIDDIMS = 256\n",
    "TAKE_EVERY_TIME_STEP_OUTPUTS = False\n",
    "\n",
    "#%% load data\n",
    "data = joblib.load(\"/home/chadyang/CEDL/final/Features/SUPERSEDED-The-Voice-Conversion-Challenge-2016/pkl2/trainFea-2people.pkl\")\n",
    "s_data = data[:20363]\n",
    "t_data = data[20363:]\n",
    "\n",
    "sid = joblib.load(\"/home/chadyang/CEDL/final/Features/SUPERSEDED-The-Voice-Conversion-Challenge-2016/pkl2/trainLabel-2people-2.pkl\")\n",
    "s_sid=sid[:20363][:64]\n",
    "t_sid=sid[20363:][:64]\n",
    "\n",
    "s_sid = torch.from_numpy(s_sid).float()\n",
    "t_sid = torch.from_numpy(t_sid).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_logp = []\n",
    "#%% train\n",
    "for iter in range(ALLITER):\n",
    "    if isinstance(loss_logp, np.ndarray):\n",
    "        loss_logp = loss_logp.tolist()\n",
    "\n",
    "    # E init\n",
    "    netAE = LSTMAE() # input: s_data: 64*513\n",
    "    netAE.apply(weights_init).cuda()\n",
    "\n",
    "    # mse loss\n",
    "    mse_s = nn.MSELoss().cuda()\n",
    "    mse_t = nn.MSELoss().cuda()\n",
    "\n",
    "    #backward parameter\n",
    "    one = torch.FloatTensor([1]).cuda()\n",
    "    mone = one * -1\n",
    "\n",
    "    # setup optimizer\n",
    "    #optimizerD = optim.Adam(netD.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    optimizerE = optim.Adam(netE.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "\n",
    "    \n",
    "    #==============================================================================\n",
    "    # (2) Update G network: VAE\n",
    "    #==============================================================================\n",
    "    real_data_t = generate_random_sample(t_data).__next__()\n",
    "    real_data_t_v =Variable(real_data_t).cuda()\n",
    "    real_id_t_v =Variable(t_sid).cuda()\n",
    "\n",
    "    real_data_s = generate_random_sample(s_data).__next__()\n",
    "    real_data_s_v = Variable(real_data_s).cuda()\n",
    "    real_id_s_v = Variable(s_sid).cuda()\n",
    "    \n",
    "    #MSE\n",
    "    #rec_s = netG(sampler(netE(real_data_s_v)), real_id_s_v)\n",
    "    rec_s = netG(netE(real_data_s_v), real_id_s_v)\n",
    "    MSEerr_s = mse_s(rec_s, real_data_s_v)\n",
    "#    log_var_s = Variable(torch.FloatTensor(np.zeros(real_data_s_v.data.shape)).cuda())\n",
    "#    logp_s = GaussianLogDensity(real_data_s_v, rec_s, log_var_s)\n",
    "\n",
    "    #rec_t = netG(sampler(netE(real_data_t_v)), real_id_t_v)\n",
    "    rec_t = netG(netE(real_data_t_v), real_id_t_v)\n",
    "    MSEerr_t = mse_t(rec_t, real_data_t_v)\n",
    "#    log_var_t = Variable(torch.FloatTensor(np.zeros(rec_s.data.shape)).cuda())\n",
    "#    logp_t = GaussianLogDensity(real_data_t_v, rec_t, log_var_t)\n",
    "\n",
    "    MSEerr = (MSEerr_s +  MSEerr_t) / 2.0\n",
    "#    MSEerr = (logp_s + logp_t) / -2.0\n",
    "    loss_logp.append(MSEerr.data[0])\n",
    "    \n",
    "    if iter%500==0:\n",
    "    #print(iter, ' | W:{} | KLD:{} | MSE:{}'.format(round((Wasserstein_D).data[0],3), round(KLD.data[0],3), round(MSEerr.data[0],3)))\n",
    "        print(iter, ' | MSE:{}'.format(round(MSEerr.data[0],3)))\n",
    "\n",
    "    if iter<VAEITER:\n",
    "        # Update E\n",
    "        #sampler.zero_grad()\n",
    "        netE.zero_grad()\n",
    "        #VAEerr = KLD + MSEerr\n",
    "        VAEerr = MSEerr\n",
    "        VAEerr.backward(retain_graph=True)\n",
    "        optimizerE.step()\n",
    "\n",
    "        # Update G\n",
    "        netG.zero_grad()\n",
    "        # G_loss = ALPHA*(-D_cost)+MSEerr\n",
    "        G_loss = MSEerr\n",
    "        G_loss.backward()\n",
    "        optimizerG.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
